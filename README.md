# Reinforcement-Learning-Agent-Escape-the-Castle


This project implements reinforcement learning algorithms to solve **Escape the Castle**, a grid-based survival game inspired by Markov Decision Processes (MDPs). The player must navigate a 5Ã—5 grid, avoid hidden guards, and reach the goal while managing health and decision-making under uncertainty.

## ğŸ” Techniques Used

- **Model-Based Monte Carlo (MBMC):** Estimated outcome probabilities (e.g., chance of defeating guards) through environment sampling.
- **Model-Free Monte Carlo (Q-Learning):** Trained an agent using an Îµ-greedy strategy with dynamic learning rates and reward-based updates.
- **Dimensionality Reduction:** Used for visualizing state representations (e.g., PCA, t-SNE).
- **MDP Simulation:** Designed a state-action-reward structure with stochastic transitions and partial observability.

## âš™ï¸ Core Files

- `MBMC.py` â€“ Estimates guard victory probabilities via sampling.
- `MFMC.py` â€“ Implements Q-learning and outputs a trained Q-table.
- `mdp_gym.py` â€“ Contains the environment logic with the `env.step()` interface.
- `vis_gym.py` â€“ Pygame-based visualization of gameplay and agent behavior.


